# clip-coca-image-text

#### COCO 
COCO(Common Objects in Context) is a large-scale image recognition, segmentation, and captioning dataset. The COCO dataset is widely used for training and evaluating computer vision models, and is often used as a benchmark for measuring the performance of different computer vision tasks.

#### CLIP 
CLIP (Contrastive Language-Image Pre-Training) is a state-of-the-art deep learning model developed by OpenAI for natural language understanding and computer vision tasks. It is designed to understand both text and images in a joint embedding space, where each image and its associated text description are mapped to a shared space of vectors.

##### INSTALLATION
```#@title Install
!pip install transformers
!pip install git+https://github.com/openai/CLIP.git```
